{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "<br>\n",
    "\n",
    "# Introdução\n",
    "\n",
    "Esse *script* tem como objetivo obter os dados sobre mananciais gerenciados pela [SABESP](http://site.sabesp.com.br), que abastecem a Região Metropolitana de São Paulo (RMSP). A gestão desses dados é feita pelo Laboratório de Sistemas de Suporte a Decisões em Engenharia Ambiental e de Recursos Hídricos ([LabSid](http://www.labsid.eng.br)), que dá suporte ao Sistema de Suporte a Decisões [SSD](http://ssd3sabesp.labsid.eng.br), bem como publiciza os dados no [*site* da SABESP](http://mananciais.sabesp.com.br/HistoricoSistemas).\n",
    "\n",
    "<br>\n",
    "\n",
    "O *script* foi desenvolvido a partir da interpretação do arquivo *json* que alimenta o *site*, identificado inspecionando a página. Funções específicas foram criadas para obter cada conjunto de dados (conforme o arquivo *json* se encontra separado). Ao final, para obtenção dos resultados, as funções são aplicadas em conjunto, otendo-se os dados para um intervalo entre datas, ou ainda, uma data única, com a finalidade de uma atualização diária.\n",
    "![json](https://i.imgur.com/nvlAMuz.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "***TODO***: Criar *script* para consumir esses dados:\n",
    "- http://mananciais.sabesp.com.br/api/Mananciais/ResumoTelemetricos/2020-04-01\n",
    "\n",
    "***TODO***: O Presente código tem a finalidade de obter os dados unicamente do Sistema Cantareira e, portanto, não se pensou em adicionar a possibilidade de obter os dados dos outros sistemas produtores, por meio da inclusão e ajuste desse parâmetro definido e fixado como *0*.\n",
    "\n",
    "Inicialmente importa-se as bibliotecas que serão necessárias para rodar os códigos abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import zlib\n",
    "import requests\n",
    "import calendar\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "E é criada a estrutura de pastas que será utilizada para armazenar os arquivos que serão criados ao longo do processo. Em específico, a pasta *data* é a que armazenará a tabela com as informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join('..', 'data')\n",
    "docs_path = os.path.join('..', 'docs')\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(docs_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "# Série Histórica e Datas\n",
    "\n",
    "A função abaixo retorna três parâmetros:\n",
    "1. Tabela com uma série histórica, com uma data de início e uma data de término.\n",
    "2. Data inicial em formato texto, para ser usada no nome do arquivo que vamos criar.\n",
    "3. Data final em formato texto, para ser usada no nome do arquivo que vamos criar.\n",
    "\n",
    "Caso não seja definida uma data final, será usado a data de hoje.\n",
    "Importante ter atenção a isso pois a SABESP disponibiliza atualizações dos dados as 9:00 (conforme site deles) e, caso o presente código seja rodado entre a meia noite e as 9:00, tentar-se-á obter informações não disponíveis, provavelmente gerando erros. Para contornar isso, é necessário inserir, pelo menos, um parâmetro como *dia de hoje - 1*, por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(start=date(1970, 1, 1), end=None):\n",
    "    \"\"\"\n",
    "    Function to create date table, with only on colum named 'Data' as index.\n",
    "    With no 'end' parameter is passed, the function will return a table until today\n",
    "    With no 'start' parameter is passed, the function will return a table staring in firts day of de 70's.\n",
    "    \n",
    "    The function return two more parameters to used in filenames:\n",
    "    filename_start > first day of table \n",
    "    filename_end   > last day of table as string    \n",
    "    \"\"\"    \n",
    "    \n",
    "    if end is None:\n",
    "        end=date.today()\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Dataframe to get dates\n",
    "    df = pd.DataFrame(\n",
    "        pd.date_range(pd.to_datetime(start), end=end),\n",
    "        columns=['Data']\n",
    "    ).set_index('Data')\n",
    "\n",
    "    return (\n",
    "        df,\n",
    "        start.strftime('%Y.%m.%d'),\n",
    "        end.strftime('%Y.%m.%d'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "start = date(2021, 2, 1)\n",
    "end = date(2021, 8, 1)\n",
    "\n",
    "# Function\n",
    "df_day, filename_start, filename_end = create_df(start, end)\n",
    "\n",
    "# Results\n",
    "print(filename_start)\n",
    "print(filename_end)\n",
    "print(df_day.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "# *Link* para fazer o *download* do Json\n",
    "\n",
    "Inicialmente foram feitas tentativas diversas para melhor conhecimento do [*site* da SABESP](http://mananciais.sabesp.com.br/HistoricoSistemas?SistemaId=0) que disponibiliza as informações dos mananciais. Inicialmente tentou-se obter os dados pela técnica de *web scrapping*, até que descobriu-se que os dados são distribuídos por meio de uma API do SSD (Sistema de Suporte a Decisões).\n",
    "\n",
    "Observou-se que a consulta manual apresenta os dados do mês da presente data até o primeiro dia do mês anterior. Por exemplo, se estamos no dia 25.**03**.2020, os dados apresentados serão dessa data até o dia 01.**02**.2020, retornando aproximadamente dados de 55 dias (30 dias de um mês hipotético e 25 do outro). O mesmo padrão irá ocorrer caso a consulta seja feita em 01.**03**.2020, a qual retornar-a os dados *até o primeiro dia do mês anterior*, ou seja, 01.**02**.2020.\n",
    "\n",
    "Essa forma de \"entregar\" os dados foi considerada na requisição de dados pela API, a qual foi feita com uso do [urllib.request](https://stackoverflow.com/questions/32795460/loading-json-object-in-python-using-urllib-request-and-json-modules). A API tem seu *link* padrão apresentado abaixo, sendo inserido apenas duas variáveis: a data e o Sistema (aqui representado pelo 0 no final, que representa o Sistema Produtor Cantareira).\n",
    "\n",
    "- http://mananciais.sabesp.com.br/api/Mananciais/RepresasSistemasNivel/2020-03-25/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "url = 'http://mananciais.sabesp.com.br/api/Mananciais/RepresasSistemasNivel/{}/{}/0'.format(\n",
    "    start.strftime('%Y-%m-%d'),\n",
    "    end.strftime('%Y-%m-%d')\n",
    ")\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "# Convertendo Json para tabela e extraíndo dados\n",
    "\n",
    "Com o *link* da API com uma data definida, criou-se uma função para obter os dados em formato *json*, bastando inserir o site.\n",
    "\n",
    "Em fevereiro de 2022 fui estudar por que que o site não mais funcionada. Havia detectado isso fazia algum tempo. Notei que o primeiro erro dizia respeito ao certificado SSL. Ainda assim não tinha sucesso. Descobri que a requisição retornava um gzip, que precisava ser descompactado! [Ref](https://stackoverflow.com/questions/53934881/getting-bytes-response-from-request)\n",
    "```\n",
    "URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)>\n",
    "``` \n",
    "```python\n",
    "import zlib\n",
    "\n",
    "r = requests.get(url, verify=False)\n",
    "decompressed_data=zlib.decompress(r.content, 16+zlib.MAX_WBITS)\n",
    "data = json.loads(decompressed_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url):\n",
    "    # Get Array with data\n",
    "    #webURL = urllib.request.urlopen(url)\n",
    "    #my_bytes = webURL.read()\n",
    "\n",
    "    # Transform Array into JSON\n",
    "    #my_json = my_bytes.decode('utf8')\n",
    "    #data = json.loads(my_json)  \n",
    "    \n",
    "    r = requests.get(url, verify=False)\n",
    "    decompressed_data=zlib.decompress(r.content, 16+zlib.MAX_WBITS)\n",
    "    data = json.loads(decompressed_data)\n",
    "    return json.dumps(data, indent=2, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "jsn = get_json(url)\n",
    "#jsn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o arquivo *json* contendo todos os dados do periodo selecionado, com diversas chaves e subchaves, iniciou-se a segmentação do arquivo, convertendo o arquivo para uma tabela, com a qual tenho mais familiaridade para editar e filtrar.\n",
    "\n",
    "A função abaixo tem esse objetivo e já aproveita para excluir duas colunas que, aparentemente, não agregam informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2df(jsn):\n",
    "    # Create dataframe\n",
    "    df = pd.read_json(jsn)\n",
    "\n",
    "    # Delete columns\n",
    "    return df.drop(['FlagHasError', 'Message'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df = json2df(jsn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Sistema Produtor (*json: SistemaId*)\n",
    "\n",
    "Por meio do campo *SistemaId* é possível obter o código que define qual é o sistema produtor de água. Contudo, considerando que o presente *script* visa obter somente os dados do Sistema Cantareira, **tal função não será aplicada**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system(df):\n",
    "    # JSON to dataframe    \n",
    "    data = df.loc['SistemaId']['ReturnObj']\n",
    "    \n",
    "    # Results\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "get_system(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Data Final  (*json: DataFinal*)\n",
    "\n",
    "Identifica a data final empregada na API. Não se vislumbra muita utilidade para essa informação nesse momento, tendo em vista que foi o usuário que definiu esse parâmetro na definição do *link* de acesso à API. Logo, **tal função não será aplicada no código final**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enddate(df):\n",
    "    # JSON to dataframe    \n",
    "    data = df.loc['DataFinal']['ReturnObj']   \n",
    "   \n",
    "    # Results\n",
    "    return datetime.strptime(data, '%d/%m/%Y').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data = get_enddate(df)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Data Inicial  (*json: DataInicial*)\n",
    "\n",
    "Identifica a data inicial empregada na API. Não se vislumbra muita utilidade para essa informação nesse momento, tendo em vista que foi o usuário que definiu esse parâmetro na definição do *link* de acesso à API. Logo, **tal função não será aplicada no código final**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_startdate(df):\n",
    "    # JSON to dataframe    \n",
    "    data = df.loc['DataInicial']['ReturnObj']   \n",
    "    \n",
    "    # Results\n",
    "    return datetime.strptime(data, '%d/%m/%Y').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data = get_startdate(df)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Manobras Operacionais (*json: ListaManobras*)\n",
    "\n",
    "Identifica todas as manobras listadas no site da SABESP. São dados mais descritivos, disponibilizados visando dar mais transparência a cadeia de comando para abrir ou fechar os reservatórios. Nesse primeiro momento tais dados não serão analisado e, portanto, **tal função não será aplicada**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manobras(df):\n",
    "    # JSON to dataframe    \n",
    "    lst = df.loc['ListaManobras']['ReturnObj']\n",
    "    \n",
    "    # Results\n",
    "    return pd.json_normalize(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_manobras = get_manobras(df)\n",
    "df_manobras.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Componentes do Sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Reservatórios (*json: ListaRepresas*)\n",
    "\n",
    "A função abaixo lista os reservatórios (ou represas) do Sistema Cantareira e outras que estão integradas, de alguma maneira, ao Sistema, inserindo também o identificador de cada reservatório (*ComponenteId*).\n",
    "\n",
    "Apesar de trata-se de uma tabela que não retorna dados temporais (por exemplo: vazão, volume e chuva), ou seja, que variam ao longo do tempo, é fundamental para rotular de qual reservatório que são os dados temporais que serão obtidos nas próximas funções, visto que eles se valem, majoritariamente, do campo *ComponenteId*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_represas(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaRepresas']['ReturnObj']\n",
    "    df = pd.json_normalize(lst)\n",
    "\n",
    "    # Delete columns\n",
    "    df = df.drop(['temChuva','temNivel', 'temQjus', 'temQnat', 'temVolume'], axis=1)\n",
    "\n",
    "    # Results\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_represas = list_represas(df)\n",
    "df_represas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Estruturas (Túneis e outros Pontos de Medição)  (*json: ListaLocais*)\n",
    "\n",
    "A função abaixo lista os túneis e estações de monitoramento do Sistema Cantareira e outras que estão integradas, de alguma maneira, ao Sistema, inserindo também o identificador de cada local (*ComponenteId*).\n",
    "\n",
    "Apesar de trata-se de uma tabela que não retorna dados temporais (por exemplo: vazão, volume e chuva), ou seja, que variam ao longo do tempo, contudo é fundamental para rotular de qual estrutura que são os dados temporais que serão obtidos nas próximas funções, visto que eles se valem, majoritariamente, do campo *ComponenteId* ou *abreviatura*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_estruturas(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaLocais']['ReturnObj']\n",
    "    df = pd.json_normalize(lst)\n",
    "\n",
    "    # Delete columns\n",
    "    df.drop(\n",
    "        ['Maximo','Minimo',\n",
    "         'Data','Dia',\n",
    "         'Valor','Unidade'],\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Transform columns to list and reorder list\n",
    "    col = df.columns.to_list()\n",
    "    col.insert(0, col.pop(col.index('ComponenteId')))\n",
    "\n",
    "    # Reindex Columns\n",
    "    df = df.reindex(columns=col)\n",
    "\n",
    "    # Results\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_estruturas = list_estruturas(df)\n",
    "df_estruturas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Dados Diários\n",
    "\n",
    "Nessa seção que serão obtidos diversos dados relevantes na operação do Sistema Cantareira, tais como:\n",
    "- Vazão natural em cada reservatório;\n",
    "- Vazão afluente em cada reservatório;\n",
    "- Vazão defluente em cada reservatório;\n",
    "- Nível e Volume de cada reservatório;\n",
    "- Dados de Precipitação de cada reservatório.\n",
    "\n",
    "Inicialmente, definiu-se uma função para renomear *strings*, visto que estas constarão nos cabeçalhos das tabelas a serem criadas. Aplicou-se a função na *df_represas* (criada acima) apenas para observar quais serão os nomes que constarão no cabeçalho das tabelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_field(x):\n",
    "    return(x.replace('/', '-').\n",
    "           replace(' (', '-').\n",
    "           replace('-', '_').\n",
    "           replace(')', '').\n",
    "           replace('Cesp', 'CESP').\n",
    "           replace('Represa ', '').\n",
    "           replace(' ', '')\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Volume, QJusante e Chuva (*json: ListaDados*)\n",
    "\n",
    "Extraíndo os dados do json, por meio da chave *ListaDados* e subchave *Dados*, foi obtido os dados de volume, vazão defluente e precpitação de cada reservatório.\n",
    "\n",
    "No arquivo json, as tabelas encontravam-se empilhadas (*flat table*), com uma coluna com o nome do reservatório. Portanto, foi necessário filtrar essas tabelas por reservatório, ajustar o cabeçalho inserindo o nome do reservatório em questão e, posteriromente, fazer um join das tabelas pelo campo *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_volumes(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaDados']['ReturnObj']\n",
    "    df = pd.json_normalize(lst, 'Dados')\n",
    "\n",
    "    # Define pivot to create new tables\n",
    "    fields = df['Nome']\n",
    "    fields = sorted(list(set(fields)))\n",
    "\n",
    "    # Transform columns to list and reorder list\n",
    "    col = df.columns.to_list()\n",
    "    col.insert(0, col.pop(col.index('Data')))\n",
    "\n",
    "    # Reindex Columns\n",
    "    df = df.reindex(columns=col)\n",
    "\n",
    "    # Create a blank table\n",
    "    df_full,start,end = create_df()\n",
    "\n",
    "    for i in fields:\n",
    "        # Define Nomes e Nomes de Tabelas\n",
    "        j = rename_field(i)\n",
    "        df_name = 'df_dados_{}'.format(j)\n",
    "        \n",
    "        # Filtra e cria das tabelas por fields (represas)\n",
    "        locals()[df_name] = df[df['Nome'] == i]\n",
    "\n",
    "        # Deleta colunas\n",
    "        locals()[df_name] = locals()[df_name].drop(['FlagConsolidado',\n",
    "                                                    'NAMaxMax','NAMinMin',\n",
    "                                                    'QJusanteMax','QJusanteMin',\n",
    "                                                    'NivelUltimoDia',\n",
    "                                                    'SistemaId','ComponenteId',\n",
    "                                                    'UltimoDia',\n",
    "                                                    'VazaoJusantePrincipal','VazaoJusanteSecundaria',\n",
    "                                                    'VolumeOperacionalUltimoDia','VolumePorcentagemUltimoDia',\n",
    "                                                    'VolumeTotalUltimoDia','Nome'], axis=1)\n",
    "\n",
    "        # Renomeia as colunas\n",
    "        locals()[df_name].columns = [x if x=='Data' else j+'_'+x for x in locals()[df_name].columns]\n",
    "\n",
    "        # Convert Data Column (object) to datatime colum\n",
    "        locals()[df_name]['Data'] = pd.to_datetime(locals()[df_name]['Data'])\n",
    "\n",
    "        # Merge all tables\n",
    "        df_full = pd.merge(df_full, locals()[df_name], on='Data', how='left')\n",
    "\n",
    "    # Results\n",
    "    df_full = df_full.set_index('Data')\n",
    "    df_full.dropna(how='all', inplace=True)\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_volumes = list_volumes(df)\n",
    "df_volumes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vazão Afluente e Vazão Natural  (*json: ListaDados*)\n",
    "\n",
    "Extraíndo os dados do json, por meio da chave *ListaDados* e subchave *Qnat*, foram obtido os dados de vazão afluente e vazão naturalde cada reservatório.\n",
    "\n",
    "No arquivo json, as tabelas encontravam-se empilhadas (*flat table*), com uma coluna com o nome do reservatório. Portanto, foi necessário filtrar essas tabelas por reservatório, ajustar o cabeçalho inserindo o nome do reservatório em questão e, posteriromente, fazer um join das tabelas pelo campo *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_vazao(df):\n",
    "    # Represas\n",
    "    df_represas = list_represas(df)\n",
    "    \n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaDados']['ReturnObj']\n",
    "    df = pd.json_normalize(lst, 'Qnat')\n",
    "\n",
    "    # Merge Tables\n",
    "    df = pd.merge(df, df_represas, on='ComponenteId', how='outer')\n",
    "\n",
    "    # Define pivot to create new tables\n",
    "    fields = df['Nome']\n",
    "    fields = sorted(list(set(fields)))\n",
    "    \n",
    "    # Transform columns to list and reorder list\n",
    "    col = df.columns.to_list()\n",
    "    col.insert(0, col.pop(col.index('Data')))\n",
    "\n",
    "    # Reindex Columns\n",
    "    df = df.reindex(columns=col)\n",
    "\n",
    "    # Create a blank table\n",
    "    df_full,start,end = create_df()\n",
    "\n",
    "    for i in fields:\n",
    "        # Define Nomes e Nomes de Tabelas\n",
    "        j = rename_field(i)\n",
    "        df_name = 'df_vazao'+'_'+j\n",
    "        \n",
    "        # Filtra e cria das tabelas por fields (represas)\n",
    "        locals()[df_name] = df[df['Nome'] == i]\n",
    "\n",
    "        # Deleta colunas\n",
    "        locals()[df_name] = locals()[df_name].drop(['ComponenteId','Nome',\n",
    "                                                    'VazaoAfluenteMax','VazaoAfluenteMin',\n",
    "                                                    'VazaoNaturalMax','VazaoNaturalMin'], axis=1)\n",
    "\n",
    "        # Renomeia as colunas\n",
    "        locals()[df_name].columns = [x if x=='Data' else j+'_'+x for x in locals()[df_name].columns]\n",
    "\n",
    "        # Convert Data Column (object) to datatime colum\n",
    "        locals()[df_name]['Data'] = pd.to_datetime(locals()[df_name]['Data'])\n",
    "\n",
    "        # Merge all tables\n",
    "        df_full = pd.merge(df_full,locals()[df_name], on='Data', how='left')\n",
    "\n",
    "    # Results\n",
    "    df_full = df_full.set_index('Data')\n",
    "    df_full.dropna(how='all', inplace=True)\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_vazao = list_vazao(df)\n",
    "df_vazao.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Sistema Equivalente  (*json: ListaDados*)\n",
    "\n",
    "Extraíndo os dados do *json*, por meio da chave *ListaDados*, foram obtido os dados do Sistema Equivalente.\n",
    "A SABESP entendo que o Sistema Equivalente é a somatária das represas que estão na Bacia do rio Piracicaba, ou seja:\n",
    "- Jaguari/Jacareí;\n",
    "- Cachoeira;\n",
    "- Atibainha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_SE(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaDados']['ReturnObj']\n",
    "    df = pd.json_normalize(lst)\n",
    "\n",
    "    # Delete columns\n",
    "    df = df.drop(['Dados','Data','Qnat'], axis=1)\n",
    "\n",
    "    # Transform columns to list and reorder list\n",
    "    col = df.columns.to_list()\n",
    "\n",
    "    # Functions to rename\n",
    "    col = ['SE_{}'.format(x) for x in col]\n",
    "    col = [x.replace('SistemaEquivalente.', '').replace('SE_Data', 'Data') for x in col]\n",
    "\n",
    "    # Rename Columns\n",
    "    df.columns = col\n",
    "\n",
    "    # Convert Data Column (object) to datatime colum\n",
    "    df['Data'] = pd.to_datetime(df['Data'])\n",
    "\n",
    "    # Results\n",
    "    df = df.set_index('Data')\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_SE = list_SE(df)\n",
    "df_SE.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Sistema Cantareira (*json: ListaDadosSistema*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_SC(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaDadosSistema']['ReturnObj']\n",
    "    df = pd.json_normalize(lst)\n",
    "    \n",
    "    # Delete columns\n",
    "    df = df.drop(['objSistema.SistemaId', 'objQETA', 'objSistema.Data'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Transform columns to list and reorder list\n",
    "    col = df.columns.to_list()\n",
    "\n",
    "    # Functions to rename\n",
    "    col = ['SC_{}'.format(x) for x in col]\n",
    "    col = [x.replace('objSistema', '') for x in col]\n",
    "    col = [x.replace('.', '') for x in col]\n",
    "    col = [x.replace('SC_Data', 'Data') for x in col]\n",
    "    \n",
    "    # Rename Columns\n",
    "    df.columns = col\n",
    "    \n",
    "    # Convert Data Column (object) to datatime colum\n",
    "    df['Data'] = pd.to_datetime(df['Data'])\n",
    "\n",
    "    # Results\n",
    "    df = df.set_index('Data')\n",
    "    df.dropna(how='all', inplace=True)\n",
    "\n",
    "    # Results\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_SC = list_SC(df)\n",
    "df_SC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Vazão dos Túneis e outros Pontos de Medição (*json: ListaDadosLocais*)\n",
    "\n",
    "Extraíndo os dados do json, por meio da chave *ListaDadosLocais* e subchave *Dados*, foram obtido os dados de vazão dos túneis Q7, Q6 Q5 e outros.\n",
    "\n",
    "No arquivo json, as tabelas encontravam-se empilhadas (*flat table*), com uma coluna com o nome da estrutura. Portanto, foi necessário filtrar essas tabelas por estrutura, adicionando ao cabeçalho seu repectivo nome e, posteriormente, fazer um join das tabelas pelo campo *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_vazaoestruturas(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaDadosLocais']['ReturnObj']\n",
    "    # Descobri que deixou de funcionar pois a partir de 01.06.2021 teve falhas no Q\"SC-PS\"\n",
    "    #df = pd.json_normalize(lst, 'Dados')\n",
    "    list_d = []\n",
    "    for parte1 in lst:\n",
    "        for parte2 in parte1['Dados']:    \n",
    "            if isinstance(parte2, dict):\n",
    "                list_d.append(parte2)\n",
    "    df = pd.DataFrame(list_d)\n",
    "\n",
    "    # Define pivot to create new tables\n",
    "    fields = df['Abreviatura']\n",
    "    fields = sorted(list(set(fields)))\n",
    "\n",
    "    # Transform columns to list and reorder list\n",
    "    col = df.columns.to_list()\n",
    "    col.insert(0, col.pop(col.index('Data')))\n",
    "    col.append(col.pop(col.index('Unidade')))\n",
    "\n",
    "    # Reindex Columns\n",
    "    df = df.reindex(columns=col)\n",
    "\n",
    "    # Create a blank table\n",
    "    df_full,start,end = create_df()\n",
    "\n",
    "    for i in fields:\n",
    "        # Define Nomes e Nomes de Tabelas\n",
    "        j = rename_field(i)\n",
    "        df_name = 'df_dados_{}'.format(j)\n",
    "        \n",
    "        # Filtra e cria das tabelas por fields (represas)\n",
    "        locals()[df_name] = df[df['Abreviatura'] == i].copy()\n",
    "        #locals()[df_name] = df.loc[:, 'Abreviatura' == i]\n",
    "\n",
    "        # Deleta colunas\n",
    "        locals()[df_name].drop(\n",
    "            [\n",
    "                'Maximo', 'Minimo', 'Dia',\n",
    "                'Abreviatura', 'ComponenteId',\n",
    "                'LocalMedicaoId', 'Nome', 'SistemaId'\n",
    "            ],\n",
    "            axis=1,\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        # Renomeia as colunas\n",
    "        locals()[df_name].columns = [x if x=='Data' else '{}_{}'.format(j, x) for x in locals()[df_name].columns]\n",
    "\n",
    "        # Convert Data Column (object) to datatime colum\n",
    "        #locals()[df_name]['Data'] = pd.to_datetime(locals()[df_name]['Data'])\n",
    "        locals()[df_name].loc[:, 'Data'] = pd.to_datetime(locals()[df_name]['Data'])\n",
    "\n",
    "        # Merge all tables\n",
    "        df_full = pd.merge(df_full,locals()[df_name], on='Data', how='left')\n",
    "\n",
    "    # Results\n",
    "    df_full.set_index('Data', inplace=True)\n",
    "    df_full.dropna(how='all', inplace=True)\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_vazaoestruturas = list_vazaoestruturas(df)\n",
    "df_vazaoestruturas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Dados Horários\n",
    "\n",
    "Referem-se a transposição da bacia do rio Paraíba do Sul para a bacia do rio Piracicaba, por meio da Estação Elevatória de Água Bruta (EEAB) Jaguari, que despeja água na represa Atibainha.\n",
    "\n",
    "Tais dados não serão aqui considerados, visto que já se encontram discretizados em dado diário na tabela acima. Logo, **tal função não será aplicada**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Estrutura de Transposição (*json: ListaEspecial*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_EEAB(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaEspecial']['ReturnObj']\n",
    "    df = pd.json_normalize(lst)\n",
    "    \n",
    "    # Results\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_vazaoEEAB_pontos = list_EEAB(df)\n",
    "df_vazaoEEAB_pontos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Vazão de Transposição  (*json: ListaDadosEspecial*)\n",
    "\n",
    "Dados horários, transferência "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_vazao_EEAB(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaDadosEspecial']['ReturnObj']\n",
    "    df = pd.json_normalize(lst, 'Dados')\n",
    "    \n",
    "    # Results\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_vazaoEEAB = list_vazao_EEAB(df)\n",
    "df_vazaoEEAB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Dados das ETAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Lista das ETAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_etas(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaETAs']['ReturnObj']\n",
    "    df = pd.json_normalize(lst)\n",
    "\n",
    "    # Results\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_etas = list_etas(df)\n",
    "df_etas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dados das ETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_etas_dados(df):\n",
    "    # JSON to dataframe\n",
    "    lst = df.loc['ListaDadosSistema']['ReturnObj']\n",
    "    df = pd.json_normalize(lst, 'objQETA')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_etas_dados = list_etas_dados(df)\n",
    "df_etas_dados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Resultado: Série Histórica do Sistema Cantareira\n",
    "\n",
    "Com todas as funções definidas, é possível aplicar tais funções sequencialmente, visando criar uma série histórica com todos os dados do Sistema Cantareira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define data de início e fim\n",
    "start = date(2021, 1, 1)\n",
    "end   = date(2021, 9, 30)\n",
    "#end   = datetime.date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Roda a primeira função para pegar a tabela com datas\n",
    "df_day, filename_start, filename_end = create_df(start, end)\n",
    "\n",
    "# Years's List\n",
    "list_year = df_day.index.year\n",
    "list_year = list(set(list_year))\n",
    "list_year = sorted(list_year, reverse=True)\n",
    "print(list_year)\n",
    "\n",
    "# Zera os Objetos\n",
    "dfs_volumes         = []\n",
    "dfs_vazao           = []\n",
    "dfs_SE              = []\n",
    "dfs_SC              = []\n",
    "dfs_vazaoestruturas = []\n",
    "\n",
    "# Function to loop\n",
    "for y in list_year:\n",
    "    # Tempo\n",
    "    print('Início do ano {} as {}'.format(y, datetime.now().strftime('%H:%M:%S')))\n",
    "    \n",
    "    # Variáveis de Data\n",
    "    firstdayyear   = date(y, 1, 1)\n",
    "    lastdayyear    = date(y, 12, 31)\n",
    "    today          = date.today()\n",
    "    \n",
    "    if today < lastdayyear:\n",
    "        lastday = today\n",
    "    elif today >= lastdayyear:\n",
    "        lastday = lastdayyear\n",
    "        \n",
    "    # Url\n",
    "    url = 'http://mananciais.sabesp.com.br/api/Mananciais/RepresasSistemasNivel/{}/{}/0'.format(\n",
    "        firstdayyear.strftime('%Y-%m-%d'),\n",
    "        lastday.strftime('%Y-%m-%d')\n",
    "    )\n",
    "    \n",
    "    # Get json\n",
    "    jsn                 = get_json(url)\n",
    "    time.sleep(3)\n",
    "    df                  = json2df(jsn)\n",
    "    \n",
    "    # Data    \n",
    "    system              = get_system(df)\n",
    "    startdate           = get_startdate(df)\n",
    "    enddate             = get_enddate(df)    \n",
    "    df_manobras         = get_manobras(df)\n",
    "    \n",
    "    # Represas\n",
    "    df_represas         = list_represas(df)\n",
    "    \n",
    "    # Dados\n",
    "    df_volumes          = list_volumes(df)\n",
    "    df_vazao            = list_vazao(df)\n",
    "    df_SE               = list_SE(df)\n",
    "    df_SC               = list_SC(df)\n",
    "    df_vazaoestruturas  = list_vazaoestruturas(df)\n",
    "    \n",
    "    # Concat Data\n",
    "    dfs_volumes.append(df_volumes)\n",
    "    dfs_vazao.append(df_vazao)\n",
    "    dfs_SE.append(df_SE)\n",
    "    dfs_SC.append(df_SC)\n",
    "    dfs_vazaoestruturas.append(df_vazaoestruturas)\n",
    "    \n",
    "    print('Fim')\n",
    "    \n",
    "# Time\n",
    "print('Fim as {}'.format(datetime.now().strftime('%H:%M:%S')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Conctatena e une tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Concat Data\n",
    "dfs_volumes         = pd.concat(dfs_volumes)\n",
    "dfs_vazao           = pd.concat(dfs_vazao)\n",
    "dfs_SE              = pd.concat(dfs_SE)\n",
    "dfs_SC              = pd.concat(dfs_SC)\n",
    "dfs_vazaoestruturas = pd.concat(dfs_vazaoestruturas)\n",
    "\n",
    "# One Table\n",
    "dfs_volumes = pd.concat([dfs_volumes, dfs_vazao], axis=1)\n",
    "dfs_volumes = pd.concat([dfs_volumes, dfs_SE], axis=1)\n",
    "dfs_volumes = pd.concat([dfs_volumes, dfs_SC], axis=1)\n",
    "dfs_volumes = pd.concat([dfs_volumes, dfs_vazaoestruturas], axis=1)\n",
    "\n",
    "# Merge\n",
    "df = pd.merge(df_day, dfs_volumes, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Rename\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(str.lower, axis='columns', inplace=True)\n",
    "df = df.rename(\n",
    "    columns=lambda x: x\n",
    "    .replace('á', 'a')\n",
    "    .replace('é', 'e')\n",
    "    .replace('í', 'i')\n",
    "    .replace('ó', 'o')\n",
    "    .replace('ú', 'u')\n",
    "    .replace('ã', 'a')\n",
    "    .replace('ç', 'c')\n",
    ")\n",
    "\n",
    "# Results\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Export\n",
    "df.dropna(how='all', inplace=True)\n",
    "\n",
    "df.to_csv(\n",
    "    os.path.join(data_path, 'tab_Cantareira_{}__{}.csv'.format(filename_start, filename_end)),\n",
    "    index=False,\n",
    "    header=True,\n",
    "    encoding='UTF-8-SIG',\n",
    "    sep=';',\n",
    "    decimal=',',\n",
    "    date_format='%d/%m/%Y',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "<br>\n",
    "\n",
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from traitlets.config import Config\n",
    "from nbconvert import PythonExporter\n",
    "from nbconvert.preprocessors import TagRemovePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "input_filename = '1_get_data.ipynb'\n",
    "input_filepath = os.path.join(os.getcwd(), input_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the exporter\n",
    "c = Config()\n",
    "c.TagRemovePreprocessor.enabled=True\n",
    "c.ClearOutputPreprocessor.enabled=True\n",
    "c.TemplateExporter.exclude_markdown=True\n",
    "c.TemplateExporter.exclude_code_cell=False\n",
    "c.TemplateExporter.exclude_input_prompt=True\n",
    "c.TemplateExporter.exclude_output=True\n",
    "c.TemplateExporter.exclude_raw=True\n",
    "c.TagRemovePreprocessor.remove_cell_tags = ('remove_cell',)\n",
    "c.TagRemovePreprocessor.remove_input_tags = ('remove_cell',)\n",
    "c.TagRemovePreprocessor.remove_all_outputs_tags = ('remove_output',)\n",
    "c.preprocessors = ['TagRemovePreprocessor']\n",
    "c.PythonExporter.preprocessors = ['nbconvert.preprocessors.TagRemovePreprocessor']\n",
    "\n",
    "# Configure and run out exporter\n",
    "py_exporter = PythonExporter(config=c)\n",
    "py_exporter.register_preprocessor(TagRemovePreprocessor(config=c), True)\n",
    "\n",
    "# Configure and run out exporter - returns a tuple - first element with html, second with notebook metadata\n",
    "body, metadata = PythonExporter(config=c).from_filename(input_filepath)\n",
    "\n",
    "# Write to output html file\n",
    "with open(os.path.join(os.getcwd(), '..', 'src', 'get_data.py'),  'w', encoding='utf-8') as f:\n",
    "    f.write(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pablocarreira-py39] *",
   "language": "python",
   "name": "conda-env-pablocarreira-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
